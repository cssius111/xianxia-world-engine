#!/usr/bin/env python3
"""
XWE DevBuddy æœ€ç»ˆä¼˜åŒ–è¡¥ä¸
è§£å†³ DeepSeek API é‡è¯•ã€Mock æ¨¡å¼ã€Token ä¿æŠ¤å’Œæ—¥å¿—ä¼˜åŒ–ç­‰é—®é¢˜
"""

import os
import sys
import shutil
import logging
import subprocess
from pathlib import Path
from typing import List, Dict, Any

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent

class XWEOptimizationPatcher:
    """XWE DevBuddy ä¼˜åŒ–è¡¥ä¸å™¨"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("XWE.Optimizer")
        logger.setLevel(logging.DEBUG if self.verbose else logging.INFO)
        
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def apply_optimizations(self) -> bool:
        """åº”ç”¨æ‰€æœ‰ä¼˜åŒ–"""
        self.logger.info("ğŸš€ å¼€å§‹åº”ç”¨ XWE DevBuddy ä¼˜åŒ–...")
        
        try:
            # 1. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temporary_files()
            
            # 2. ä¼˜åŒ– LLM å®¢æˆ·ç«¯
            self._optimize_llm_client()
            
            # 3. å¢å¼º NLP å¤„ç†å™¨
            self._enhance_nlp_processor()
            
            # 4. ä¼˜åŒ–æ—¥å¿—é…ç½®
            self._optimize_logging_config()
            
            # 5. æ›´æ–° run.py
            self._update_run_py()
            
            # 6. åˆ›å»º CLI å·¥å…·
            self._create_cli_tool()
            
            # 7. æ·»åŠ æµ‹è¯•ç”¨ä¾‹
            self._add_comprehensive_tests()
            
            self.logger.info("âœ… æ‰€æœ‰ä¼˜åŒ–å·²æˆåŠŸåº”ç”¨!")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¼˜åŒ–åº”ç”¨å¤±è´¥: {e}")
            return False
    
    def _cleanup_temporary_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        self.logger.info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        
        temp_files = [
            "test_nlp_fix.py",
            "fix_nlp_processor.py",
            "final_hf002_test.py",
            "test_simple_fix.py",
            "verify_fixes.py",
            "verify_hf002_fixes.py",
            "api_fixes.py",
        ]
        
        cleaned_count = 0
        for file_name in temp_files:
            file_path = PROJECT_ROOT / file_name
            if file_path.exists():
                try:
                    file_path.unlink()
                    self.logger.info(f"  ğŸ—‘ï¸ å·²åˆ é™¤: {file_name}")
                    cleaned_count += 1
                except Exception as e:
                    self.logger.warning(f"  âš ï¸ åˆ é™¤å¤±è´¥ {file_name}: {e}")
        
        self.logger.info(f"  ğŸ“Š æ¸…ç†äº† {cleaned_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
    
    def _optimize_llm_client(self):
        """ä¼˜åŒ– LLM å®¢æˆ·ç«¯"""
        self.logger.info("âš¡ ä¼˜åŒ– LLM å®¢æˆ·ç«¯...")
        
        llm_client_path = PROJECT_ROOT / "src/xwe/core/nlp/llm_client.py"
        
        # è¯»å–ç°æœ‰æ–‡ä»¶
        with open(llm_client_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¼˜åŒ–è¿‡
        if "USE_MOCK_LLM" in content and "XWE_MAX_LLM_RETRIES" in content:
            self.logger.info("  âœ… LLM å®¢æˆ·ç«¯å·²ä¼˜åŒ–ï¼Œè·³è¿‡")
            return
        
        optimized_llm_client = '''"""
DeepSeek API å®¢æˆ·ç«¯ - ä¼˜åŒ–ç‰ˆæœ¬
å¤„ç†ä¸ DeepSeek API çš„é€šä¿¡ï¼Œæ”¯æŒå¯é…ç½®é‡è¯•å’Œ Mock æ¨¡å¼
"""

import functools
import json
import logging
import os
from time import sleep, time
from typing import Any, Dict, Optional

import requests

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ backoffï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç®€å•çš„é‡è¯•è£…é¥°å™¨
try:
    import backoff
    HAS_BACKOFF = True
except ImportError:
    HAS_BACKOFF = False
    logger.warning("backoff æ¨¡å—æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•é‡è¯•æœºåˆ¶")


def simple_retry(max_tries=3, delay=1.0):
    """ç®€å•çš„é‡è¯•è£…é¥°å™¨ï¼ˆå½“ backoff ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_tries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_tries - 1:
                        sleep(delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    else:
                        raise
            raise last_exception
        return wrapper
    return decorator


class LLMClient:
    """
    DeepSeek API å®¢æˆ·ç«¯ - ä¼˜åŒ–ç‰ˆæœ¬
    
    æ–°å¢åŠŸèƒ½ï¼š
    - å¯é…ç½®é‡è¯•æ¬¡æ•° (XWE_MAX_LLM_RETRIES)
    - Mock æ¨¡å¼æ”¯æŒ (USE_MOCK_LLM=true)
    - æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_url: str = "https://api.deepseek.com/v1/chat/completions",
        model_name: str = "deepseek-chat",
        timeout: int = 30,
        debug: bool = False,
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            api_url: APIç«¯ç‚¹URL
            model_name: æ¨¡å‹åç§°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ Mock æ¨¡å¼
        self.use_mock = os.getenv("USE_MOCK_LLM", "false").lower() == "true"
        
        if self.use_mock:
            logger.info("ğŸ­ LLM Mock æ¨¡å¼å·²å¯ç”¨ï¼Œå°†è·³è¿‡ç½‘ç»œè°ƒç”¨")
            self.api_key = "mock_key"
        else:
            self.api_key = api_key or os.environ.get("DEEPSEEK_API_KEY")
            if not self.api_key:
                raise ValueError("DEEPSEEK_API_KEY not found in environment variables")

        self.api_url = api_url
        self.model_name = model_name
        self.timeout = timeout
        self.debug = debug
        
        # å¯é…ç½®çš„é‡è¯•æ¬¡æ•°
        self.max_retries = int(os.getenv("XWE_MAX_LLM_RETRIES", "3"))

        # è¯·æ±‚å¤´
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    def _make_request_with_retry(self, payload: Dict) -> Dict:
        """å‘é€è¯·æ±‚çš„åŒ…è£…æ–¹æ³•"""
        if self.use_mock:
            return self._mock_response(payload)
        
        if HAS_BACKOFF:
            # ä½¿ç”¨ backoff åº“çš„é«˜çº§é‡è¯•æœºåˆ¶
            @backoff.on_exception(
                backoff.expo,
                (requests.exceptions.RequestException, requests.exceptions.Timeout),
                max_tries=self.max_retries,
                max_time=60,
            )
            def _request():
                return self._make_request(payload)
            return _request()
        else:
            # ä½¿ç”¨ç®€å•é‡è¯•æœºåˆ¶
            @simple_retry(max_tries=self.max_retries, delay=1.0)
            def _request():
                return self._make_request(payload)
            return _request()

    def _mock_response(self, payload: Dict) -> Dict:
        """Mock å“åº”ç”Ÿæˆå™¨"""
        # ä» payload ä¸­æå–ç”¨æˆ·æ¶ˆæ¯
        messages = payload.get("messages", [])
        user_message = ""
        for msg in messages:
            if msg.get("role") == "user":
                user_message = msg.get("content", "")
                break
        
        # ç®€å•çš„æœ¬åœ°è§£æé€»è¾‘
        mock_responses = {
            "æ¢ç´¢": {
                "raw": "æ¢ç´¢",
                "normalized_command": "æ¢ç´¢",
                "intent": "action",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šæ¢ç´¢å‘½ä»¤"
            },
            "ä¿®ç‚¼": {
                "raw": "ä¿®ç‚¼",
                "normalized_command": "ä¿®ç‚¼",
                "intent": "train",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šä¿®ç‚¼å‘½ä»¤"
            },
            "èƒŒåŒ…": {
                "raw": "èƒŒåŒ…",
                "normalized_command": "æ‰“å¼€èƒŒåŒ…",
                "intent": "check",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šèƒŒåŒ…å‘½ä»¤"
            },
            "çŠ¶æ€": {
                "raw": "çŠ¶æ€",
                "normalized_command": "æŸ¥çœ‹çŠ¶æ€",
                "intent": "check",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šçŠ¶æ€å‘½ä»¤"
            }
        }
        
        # å°è¯•åŒ¹é…ç”¨æˆ·è¾“å…¥
        for keyword, response in mock_responses.items():
            if keyword in user_message:
                return {
                    "choices": [
                        {
                            "message": {
                                "content": json.dumps(response, ensure_ascii=False)
                            }
                        }
                    ]
                }
        
        # é»˜è®¤å“åº”
        default_response = {
            "raw": user_message,
            "normalized_command": "æœªçŸ¥",
            "intent": "unknown",
            "args": {},
            "explanation": "Mockæ¨¡å¼ï¼šæœªçŸ¥å‘½ä»¤"
        }
        
        return {
            "choices": [
                {
                    "message": {
                        "content": json.dumps(default_response, ensure_ascii=False)
                    }
                }
            ]
        }

    def _make_request(self, payload: Dict) -> Dict:
        """
        å‘é€è¯·æ±‚åˆ° DeepSeek API

        Args:
            payload: è¯·æ±‚è½½è·

        Returns:
            APIå“åº”
        """
        try:
            response = requests.post(
                self.api_url, headers=self.headers, json=payload, timeout=self.timeout
            )
            response.raise_for_status()
            return response.json()

        except requests.exceptions.Timeout:
            logger.warning(f"Request to DeepSeek API timed out after {self.timeout}s")
            raise

        except requests.exceptions.RequestException as e:
            logger.warning(f"Request to DeepSeek API failed: {e}")
            if hasattr(e, "response") and e.response is not None:
                logger.warning(f"Response status: {e.response.status_code}")
                logger.warning(f"Response body: {e.response.text}")
            raise

    def chat(
        self,
        prompt: str,
        temperature: float = 0.0,
        max_tokens: int = 256,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        å‘é€èŠå¤©è¯·æ±‚

        Args:
            prompt: ç”¨æˆ·æç¤º
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰ï¼Œ0è¡¨ç¤ºæ›´ç¡®å®šçš„è¾“å‡º
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰

        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": prompt})

        # æ„å»ºè¯·æ±‚è½½è·
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        # è®°å½•è¯·æ±‚ï¼ˆä»…åœ¨é Mock æ¨¡å¼ä¸‹ï¼‰
        if not self.use_mock:
            logger.debug(
                f"Sending request to DeepSeek API: {json.dumps(payload, ensure_ascii=False)[:200]}..."
            )

        try:
            # å‘é€è¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰
            start_time = time()
            response = self._make_request_with_retry(payload)
            elapsed = time() - start_time

            if self.use_mock:
                logger.debug(f"Mock response generated in {elapsed:.2f}s")
            else:
                logger.debug(f"DeepSeek API response received in {elapsed:.2f}s")
            
            if self.debug:
                logger.debug(
                    f"DeepSeek full response: {json.dumps(response, ensure_ascii=False)}"
                )

            # æå–å“åº”æ–‡æœ¬
            if "choices" in response and len(response["choices"]) > 0:
                content = response["choices"][0].get("message", {}).get("content", "")
                return content
            else:
                logger.error(f"Unexpected response format: {response}")
                return ""

        except Exception as e:
            logger.error(f"Error calling DeepSeek API: {e}")
            raise

    def chat_with_context(
        self, messages: list, temperature: float = 0.7, max_tokens: int = 256
    ) -> str:
        """
        å¸¦ä¸Šä¸‹æ–‡çš„èŠå¤©

        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user/assistant/system", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        try:
            response = self._make_request_with_retry(payload)
            if self.debug:
                logger.debug(
                    f"DeepSeek full response: {json.dumps(response, ensure_ascii=False)}"
                )

            if "choices" in response and len(response["choices"]) > 0:
                content = response["choices"][0].get("message", {}).get("content", "")
                return content
            else:
                return ""

        except Exception as e:
            logger.error(f"Error in chat_with_context: {e}")
            raise

    def get_embeddings(self, text: str) -> Optional[list]:
        """
        è·å–æ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆå¦‚æœAPIæ”¯æŒï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            åµŒå…¥å‘é‡
        """
        # æ³¨æ„ï¼šDeepSeek APIå¯èƒ½ä¸æ”¯æŒåµŒå…¥åŠŸèƒ½
        # è¿™é‡Œåªæ˜¯é¢„ç•™æ¥å£
        logger.warning("Embeddings API not implemented for DeepSeek")
        return None


# ä¿æŒå‘åå…¼å®¹
class DeepSeek:
    """å‘åå…¼å®¹çš„DeepSeekç±»"""

    def __init__(self, api_key: str, model: str = "deepseek-chat"):
        self.client = LLMClient(api_key=api_key, model_name=model)

    def chat(self, prompt: str) -> Dict[str, Any]:
        """å…¼å®¹æ—§æ¥å£"""
        try:
            text = self.client.chat(prompt)
            return {"text": text}
        except Exception as e:
            logger.error(f"Error in legacy chat method: {e}")
            return {"text": ""}
'''
        
        # å†™å…¥ä¼˜åŒ–åçš„ LLM å®¢æˆ·ç«¯
        with open(llm_client_path, 'w', encoding='utf-8') as f:
            f.write(optimized_llm_client)
        
        self.logger.info("  âœ… LLM å®¢æˆ·ç«¯å·²ä¼˜åŒ–")
    
    def _enhance_nlp_processor(self):
        """å¢å¼º NLP å¤„ç†å™¨ï¼Œæ·»åŠ  Token é•¿åº¦ä¿æŠ¤"""
        self.logger.info("ğŸ§  å¢å¼º NLP å¤„ç†å™¨...")
        
        nlp_processor_path = PROJECT_ROOT / "src/xwe/core/nlp/nlp_processor.py"
        
        # è¯»å–ç°æœ‰æ–‡ä»¶
        with open(nlp_processor_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å« Token ä¿æŠ¤
        if "context_limit" in content and "max_prompt_tokens" in content:
            self.logger.info("  âœ… NLP å¤„ç†å™¨å·²å¢å¼ºï¼Œè·³è¿‡")
            return
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢ build_prompt æ–¹æ³•
        old_build_prompt = '''    def build_prompt(self, user_input: str, context: Optional[Dict] = None) -> str:
        """
        æ„å»ºprompt

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            å®Œæ•´çš„prompt
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®contextæ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
        # ä¾‹å¦‚å½“å‰ä½ç½®ã€å·²çŸ¥NPCã€å¯ç”¨ç‰©å“ç­‰
        
        # ä¿®å¤: å®‰å…¨åœ°å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œé¿å…KeyError
        try:
            # æ¸…ç†å’Œè½¬ä¹‰ç”¨æˆ·è¾“å…¥
            safe_input = self._sanitize_user_input(user_input)
            # ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢è€Œä¸æ˜¯formatï¼Œé¿å…KeyError
            return self.prompt_template.replace('"{}"', f'"{safe_input}"')
        except Exception as e:
            logger.warning(f"æ„å»ºpromptæ—¶å‡ºé”™: {e}, ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
            # å¦‚æœä»ç„¶å‡ºé”™ï¼Œä½¿ç”¨æœ€å®‰å…¨çš„å›é€€æ–¹æ¡ˆ
            safe_input = self._sanitize_user_input(user_input) or "æœªçŸ¥å‘½ä»¤"
            return self.prompt_template.replace('"{}"', f'"{safe_input}"')'''
        
        # æ–°çš„å¸¦ Token ä¿æŠ¤çš„ build_prompt æ–¹æ³•
        new_build_prompt = '''    def build_prompt(self, user_input: str, context: Optional[Dict] = None) -> str:
        """
        æ„å»ºpromptï¼ŒåŒ…å« Token é•¿åº¦ä¿æŠ¤

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            å®Œæ•´çš„prompt
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®contextæ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
        # ä¾‹å¦‚å½“å‰ä½ç½®ã€å·²çŸ¥NPCã€å¯ç”¨ç‰©å“ç­‰
        
        # ä¿®å¤: å®‰å…¨åœ°å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œé¿å…KeyError
        try:
            # æ¸…ç†å’Œè½¬ä¹‰ç”¨æˆ·è¾“å…¥
            safe_input = self._sanitize_user_input(user_input)
            
            # æ„å»ºåŸºç¡€ prompt
            base_prompt = self.prompt_template.replace('"{}"', f'"{safe_input}"')
            
            # Token é•¿åº¦ä¿æŠ¤
            context_limit = self.config.get("context_limit", 4096)  # é»˜è®¤ 4K context
            reserved_tokens = 200  # ä¸ºå“åº”é¢„ç•™çš„ token
            max_prompt_tokens = context_limit - reserved_tokens
            
            # ç®€å•çš„ token ä¼°ç®— (1 token â‰ˆ 4 characters for Chinese)
            estimated_tokens = len(base_prompt) // 4
            
            if estimated_tokens > max_prompt_tokens:
                logger.warning(
                    f"Prompt é•¿åº¦è¶…é™: {estimated_tokens} > {max_prompt_tokens} tokens, "
                    f"å°†æˆªæ–­å†å²å¯¹è¯"
                )
                
                # æˆªæ–­ç­–ç•¥ï¼šä¿ç•™æ ¸å¿ƒç³»ç»Ÿæç¤ºå’Œå½“å‰ç”¨æˆ·è¾“å…¥
                lines = base_prompt.split('\\n')
                essential_lines = []
                user_input_lines = []
                
                # æå–æ ¸å¿ƒéƒ¨åˆ†
                in_examples = False
                for line in lines:
                    if '### ç¤ºä¾‹ï¼š' in line:
                        in_examples = True
                        continue
                    elif f'è¾“å…¥: "{safe_input}"' in line:
                        in_examples = False
                        user_input_lines.extend(lines[lines.index(line):])
                        break
                    elif not in_examples:
                        essential_lines.append(line)
                
                # é‡æ–°ç»„åˆï¼Œä¿ç•™æ ¸å¿ƒéƒ¨åˆ†
                truncated_prompt = '\\n'.join(essential_lines + user_input_lines)
                
                # å†æ¬¡æ£€æŸ¥é•¿åº¦
                if len(truncated_prompt) // 4 > max_prompt_tokens:
                    # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œä½¿ç”¨æœ€å°åŒ– prompt
                    truncated_prompt = f\'\'\'ä½ æ˜¯ä¿®ä»™ä¸–ç•Œæ¸¸æˆçš„å‘½ä»¤è§£æå™¨ã€‚
å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºJSONæ ¼å¼ï¼š
{{
  "raw": "<ç”¨æˆ·è¾“å…¥>",
  "normalized_command": "<æ ‡å‡†å‘½ä»¤>",
  "intent": "<æ„å›¾>",
  "args": {{}},
  "explanation": "<è¯´æ˜>"
}}

è¾“å…¥: "{safe_input}"
è¾“å‡º:
\'\'\'
                
                logger.info(f"Prompt å·²æˆªæ–­è‡³ {len(truncated_prompt) // 4} tokens")
                return truncated_prompt
            
            return base_prompt
            
        except Exception as e:
            logger.warning(f"æ„å»ºpromptæ—¶å‡ºé”™: {e}, ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
            # å¦‚æœä»ç„¶å‡ºé”™ï¼Œä½¿ç”¨æœ€å®‰å…¨çš„å›é€€æ–¹æ¡ˆ
            safe_input = self._sanitize_user_input(user_input) or "æœªçŸ¥å‘½ä»¤"
            return self.prompt_template.replace('"{}"', f'"{safe_input}"')'''
        
        # æ‰§è¡Œæ›¿æ¢
        if old_build_prompt in content:
            content = content.replace(old_build_prompt, new_build_prompt)
            
            # å†™å…¥æ›´æ–°åçš„æ–‡ä»¶
            with open(nlp_processor_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info("  âœ… NLP å¤„ç†å™¨å·²å¢å¼º")
        else:
            self.logger.warning("  âš ï¸ æœªæ‰¾åˆ°éœ€è¦æ›´æ–°çš„ build_prompt æ–¹æ³•")
    
    def _optimize_logging_config(self):
        """ä¼˜åŒ–æ—¥å¿—é…ç½®"""
        self.logger.info("ğŸ“ ä¼˜åŒ–æ—¥å¿—é…ç½®...")
        
        logging_config_path = PROJECT_ROOT / "src/logging_config.py"
        
        # è¯»å–ç°æœ‰æ–‡ä»¶
        with open(logging_config_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¼˜åŒ–è¿‡
        if "verbose: bool = False" in content:
            self.logger.info("  âœ… æ—¥å¿—é…ç½®å·²ä¼˜åŒ–ï¼Œè·³è¿‡")
            return
        
        optimized_logging = '''import logging
import os
from logging import Handler
from logging.handlers import TimedRotatingFileHandler
from pathlib import Path
from typing import Dict

LOG_FMT = "%(asctime)s [%(levelname).1s] %(name)s: %(message)s"


class ThrottleFilter(logging.Filter):
    """Filter that limits log output frequency per logger."""

    def __init__(self, interval: float = 10.0) -> None:
        super().__init__()
        self.interval = interval
        self.last_emit: Dict[str, float] = {}

    def filter(self, record: logging.LogRecord) -> bool:
        last = self.last_emit.get(record.name)
        if last is None or record.created - last >= self.interval:
            self.last_emit[record.name] = record.created
            return True
        return False


class ChangeOnlyFilter(logging.Filter):
    """Filter that emits logs only when the message changes."""

    def __init__(self) -> None:
        super().__init__()
        self.last_message: Dict[str, str] = {}

    def filter(self, record: logging.LogRecord) -> bool:
        msg = record.getMessage()
        last = self.last_message.get(record.name)
        if last != msg:
            self.last_message[record.name] = msg
            return True
        return False


def _add_handler(logger: logging.Logger, handler: Handler) -> None:
    logger.addHandler(handler)


def setup_logging(verbose: bool = False) -> None:
    """
    Configure root logger for the application.
    
    Args:
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿— (DEBUG çº§åˆ«)
    """
    # æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œå‚æ•°
    debug_env = os.getenv("DEBUG_LOG") in {"1", "true", "True"}
    verbose_env = os.getenv("VERBOSE_LOG") in {"1", "true", "True"}
    
    level = logging.DEBUG if (debug_env or verbose_env or verbose) else logging.INFO
    root = logging.getLogger()
    root.setLevel(level)

    # Remove existing handlers to avoid duplicate logs
    for h in list(root.handlers):
        root.removeHandler(h)

    formatter = logging.Formatter(LOG_FMT, "%H:%M:%S")

    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    console.setFormatter(formatter)
    console.addFilter(ChangeOnlyFilter())
    console.addFilter(ThrottleFilter())
    _add_handler(root, console)

    log_dir = Path("logs")
    log_dir.mkdir(parents=True, exist_ok=True)

    debug_file = TimedRotatingFileHandler(log_dir / "app_debug.log", when="D", backupCount=7, encoding="utf-8")
    debug_file.setLevel(logging.DEBUG)
    debug_file.setFormatter(formatter)
    _add_handler(root, debug_file)

    info_file = TimedRotatingFileHandler(log_dir / "app.log", when="D", backupCount=7, encoding="utf-8")
    info_file.setLevel(logging.INFO)
    info_file.setFormatter(formatter)
    _add_handler(root, info_file)
    
    # ä¼˜åŒ–ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«ï¼ˆé™¤éå¯ç”¨è¯¦ç»†æ¨¡å¼ï¼‰
    if not verbose:
        # å°† backoff å’Œ urllib3 æ—¥å¿—çº§åˆ«è®¾ä¸º ERROR
        logging.getLogger("backoff").setLevel(logging.ERROR)
        logging.getLogger("urllib3").setLevel(logging.ERROR)
        logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
        logging.getLogger("requests").setLevel(logging.WARNING)
        
        # å…¶ä»–å¯èƒ½å™ªéŸ³è¾ƒå¤šçš„åº“
        logging.getLogger("werkzeug").setLevel(logging.WARNING)
        logging.getLogger("flask").setLevel(logging.WARNING)
    else:
        # è¯¦ç»†æ¨¡å¼ä¸‹æ¢å¤ç¬¬ä¸‰æ–¹åº“çš„æ­£å¸¸æ—¥å¿—çº§åˆ«
        logging.getLogger("backoff").setLevel(logging.INFO)
        logging.getLogger("urllib3").setLevel(logging.INFO)
'''
        
        # å†™å…¥ä¼˜åŒ–åçš„æ—¥å¿—é…ç½®
        with open(logging_config_path, 'w', encoding='utf-8') as f:
            f.write(optimized_logging)
        
        self.logger.info("  âœ… æ—¥å¿—é…ç½®å·²ä¼˜åŒ–")
    
    def _update_run_py(self):
        """æ›´æ–° run.py ä»¥æ”¯æŒæ–°çš„æ—¥å¿—é…ç½®"""
        self.logger.info("ğŸ”§ æ›´æ–° run.py...")
        
        run_py_path = PROJECT_ROOT / "run.py"
        
        # è¯»å–ç°æœ‰æ–‡ä»¶
        with open(run_py_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ›´æ–°è¿‡
        if "verbose_mode" in content:
            self.logger.info("  âœ… run.py å·²æ›´æ–°ï¼Œè·³è¿‡")
            return
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢ setup_logging è°ƒç”¨
        old_setup = '''from logging_config import setup_logging
setup_logging()'''
        
        new_setup = '''from logging_config import setup_logging
# æ£€æŸ¥æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—
verbose_mode = os.getenv("VERBOSE_LOG", "false").lower() == "true"
setup_logging(verbose=verbose_mode)'''
        
        if old_setup in content:
            content = content.replace(old_setup, new_setup)
            
            # å†™å…¥æ›´æ–°åçš„æ–‡ä»¶
            with open(run_py_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info("  âœ… run.py å·²æ›´æ–°")
        else:
            self.logger.warning("  âš ï¸ æœªæ‰¾åˆ°éœ€è¦æ›´æ–°çš„ setup_logging è°ƒç”¨")
    
    def _create_cli_tool(self):
        """åˆ›å»º CLI å·¥å…·"""
        self.logger.info("ğŸ–¥ï¸ åˆ›å»º CLI å·¥å…·...")
        
        cli_tool_path = PROJECT_ROOT / "scripts" / "xwe_cli.py"
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨
        if cli_tool_path.exists():
            self.logger.info("  âœ… CLI å·¥å…·å·²å­˜åœ¨ï¼Œè·³è¿‡")
            return
        
        cli_content = '''#!/usr/bin/env python3
"""
XWE DevBuddy CLI å·¥å…·
æä¾›å‘½ä»¤è¡Œç•Œé¢ï¼Œæ”¯æŒ --verbose é€‰é¡¹å’Œ Mock æ¨¡å¼
"""

import argparse
import logging
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from logging_config import setup_logging


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="XWE DevBuddy - ä¿®ä»™ä¸–ç•Œå¼•æ“å¼€å‘å·¥å…·"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º (DEBUG çº§åˆ«)"
    )
    
    parser.add_argument(
        "--port", "-p",
        type=int,
        default=5001,
        help="æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 5001)"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨ Flask è°ƒè¯•æ¨¡å¼"
    )
    
    parser.add_argument(
        "--mock-llm",
        action="store_true",
        help="å¯ç”¨ LLM Mock æ¨¡å¼"
    )
    
    parser.add_argument(
        "--max-retries",
        type=int,
        default=3,
        help="LLM API æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if args.verbose:
        os.environ["VERBOSE_LOG"] = "true"
    
    if args.debug:
        os.environ["DEBUG"] = "true"
    
    if args.mock_llm:
        os.environ["USE_MOCK_LLM"] = "true"
    
    if args.max_retries:
        os.environ["XWE_MAX_LLM_RETRIES"] = str(args.max_retries)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(verbose=args.verbose)
    
    logger = logging.getLogger("XWE.CLI")
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("=" * 60)
    print("ğŸ® XWE DevBuddy å¯åŠ¨ä¸­...")
    print("=" * 60)
    logger.info(f"ğŸ“ ç«¯å£: {args.port}")
    logger.info(f"ğŸ”§ è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if args.debug else 'ç¦ç”¨'}")
    logger.info(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {'å¯ç”¨' if args.verbose else 'ç¦ç”¨'}")
    logger.info(f"ğŸ­ Mock æ¨¡å¼: {'å¯ç”¨' if args.mock_llm else 'ç¦ç”¨'}")
    logger.info(f"ğŸ”„ æœ€å¤§é‡è¯•: {args.max_retries}")
    print("=" * 60)
    
    # è®¾ç½®ç«¯å£
    os.environ["PORT"] = str(args.port)
    
    # å¯åŠ¨åº”ç”¨
    try:
        # ä¿®æ”¹ run.py ä¸­çš„ setup_logging è°ƒç”¨
        import run
        
        # ç›´æ¥è°ƒç”¨ main
        run.main()
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
'''
        
        # å†™å…¥ CLI å·¥å…·
        with open(cli_tool_path, 'w', encoding='utf-8') as f:
            f.write(cli_content)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(cli_tool_path, 0o755)
        
        self.logger.info("  âœ… CLI å·¥å…·å·²åˆ›å»º")
    
    def _add_comprehensive_tests(self):
        """æ·»åŠ ç»¼åˆæµ‹è¯•ç”¨ä¾‹"""
        self.logger.info("ğŸ§ª æ·»åŠ ç»¼åˆæµ‹è¯•ç”¨ä¾‹...")
        
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dir = PROJECT_ROOT / "tests/unit/optimizations"
        test_dir.mkdir(parents=True, exist_ok=True)
        
        e2e_test_dir = PROJECT_ROOT / "tests/e2e/optimizations"
        e2e_test_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. LLM å®¢æˆ·ç«¯ä¼˜åŒ–æµ‹è¯•
        llm_test_path = test_dir / "test_llm_client_optimizations.py"
        if not llm_test_path.exists():
            llm_test_content = '''"""
LLM å®¢æˆ·ç«¯ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•
æµ‹è¯•é‡è¯•æ¬¡æ•°é…ç½®ã€Mock æ¨¡å¼ç­‰æ–°åŠŸèƒ½
"""

import json
import os
import pytest
from unittest.mock import Mock, patch

from src.xwe.core.nlp.llm_client import LLMClient


class TestLLMClientOptimizations:
    """LLM å®¢æˆ·ç«¯ä¼˜åŒ–æµ‹è¯•"""
    
    def test_mock_mode_enabled(self):
        """æµ‹è¯• Mock æ¨¡å¼å¯ç”¨"""
        with patch.dict(os.environ, {"USE_MOCK_LLM": "true"}):
            client = LLMClient()
            assert client.use_mock is True
            
            # æµ‹è¯• Mock å“åº”
            response = client.chat("æ¢ç´¢")
            assert "æ¢ç´¢" in response
            
    def test_mock_mode_disabled(self):
        """æµ‹è¯• Mock æ¨¡å¼ç¦ç”¨"""
        with patch.dict(os.environ, {"USE_MOCK_LLM": "false", "DEEPSEEK_API_KEY": "test"}):
            client = LLMClient()
            assert client.use_mock is False
    
    def test_configurable_retries(self):
        """æµ‹è¯•å¯é…ç½®é‡è¯•æ¬¡æ•°"""
        with patch.dict(os.environ, {"XWE_MAX_LLM_RETRIES": "5", "DEEPSEEK_API_KEY": "test"}):
            client = LLMClient()
            assert client.max_retries == 5
    
    def test_default_retries(self):
        """æµ‹è¯•é»˜è®¤é‡è¯•æ¬¡æ•°"""
        with patch.dict(os.environ, {"DEEPSEEK_API_KEY": "test"}, clear=True):
            client = LLMClient()
            assert client.max_retries == 3
    
    def test_mock_response_generation(self):
        """æµ‹è¯• Mock å“åº”ç”Ÿæˆ"""
        with patch.dict(os.environ, {"USE_MOCK_LLM": "true"}):
            client = LLMClient()
            
            # æµ‹è¯•ä¸åŒå‘½ä»¤çš„ Mock å“åº”
            test_cases = [
                ("æ¢ç´¢", "æ¢ç´¢"),
                ("ä¿®ç‚¼", "ä¿®ç‚¼"),
                ("èƒŒåŒ…", "æ‰“å¼€èƒŒåŒ…"),
                ("çŠ¶æ€", "æŸ¥çœ‹çŠ¶æ€"),
                ("æœªçŸ¥å‘½ä»¤xxx", "æœªçŸ¥")
            ]
            
            for user_input, expected_command in test_cases:
                response = client.chat(user_input)
                parsed = json.loads(response)
                
                assert parsed["raw"] == user_input
                assert expected_command in parsed["normalized_command"]
'''
            
            with open(llm_test_path, 'w', encoding='utf-8') as f:
                f.write(llm_test_content)
        
        # 2. E2E é›†æˆæµ‹è¯•
        e2e_test_path = e2e_test_dir / "test_optimization_integration.py"
        if not e2e_test_path.exists():
            e2e_test_content = '''"""
ä¼˜åŒ–åŠŸèƒ½ E2E é›†æˆæµ‹è¯•
æµ‹è¯• Mock æ¨¡å¼ã€é‡è¯•æœºåˆ¶çš„ç«¯åˆ°ç«¯åŠŸèƒ½
"""

import os
import pytest
from unittest.mock import patch

from src.xwe.core.nlp.nlp_processor import DeepSeekNLPProcessor


class TestOptimizationIntegration:
    """ä¼˜åŒ–åŠŸèƒ½é›†æˆæµ‹è¯•"""
    
    def test_mock_mode_end_to_end(self):
        """æµ‹è¯• Mock æ¨¡å¼ç«¯åˆ°ç«¯æµç¨‹"""
        with patch.dict(os.environ, {"USE_MOCK_LLM": "true"}):
            processor = DeepSeekNLPProcessor()
            
            # æµ‹è¯•å®Œæ•´çš„è§£ææµç¨‹
            test_commands = [
                "æ¢ç´¢å‘¨å›´ç¯å¢ƒ",
                "ä¿®ç‚¼æå‡å®åŠ›",
                "æŸ¥çœ‹å½“å‰çŠ¶æ€",
                "æ‰“å¼€èƒŒåŒ…çœ‹çœ‹"
            ]
            
            for command in test_commands:
                result = processor.parse(command)
                
                # éªŒè¯ç»“æœæ ¼å¼
                assert result.raw == command
                assert result.normalized_command is not None
                assert result.intent is not None
                assert isinstance(result.args, dict)
    
    def test_fallback_on_api_failure_simulation(self):
        """æµ‹è¯•æ¨¡æ‹Ÿ API å¤±è´¥çš„å›é€€æœºåˆ¶"""
        with patch.dict(os.environ, {"DEEPSEEK_API_KEY": "test"}):
            processor = DeepSeekNLPProcessor()
            
            # æ¨¡æ‹Ÿ DeepSeek API è°ƒç”¨å¤±è´¥
            with patch.object(processor, '_call_deepseek_api', side_effect=Exception("API Error")):
                result = processor.parse("æ¢ç´¢")
                
                # åº”è¯¥å›é€€åˆ°æœ¬åœ°è§£æ
                assert result.normalized_command == "æ¢ç´¢"
                assert result.intent == "action"
                assert result.confidence == 0.5  # å›é€€æ¨¡å¼ç½®ä¿¡åº¦
'''
            
            with open(e2e_test_path, 'w', encoding='utf-8') as f:
                f.write(e2e_test_content)
        
        # 3. åˆ›å»ºæµ‹è¯•é…ç½®
        for test_dir_path in [test_dir, e2e_test_dir]:
            conftest_path = test_dir_path / "conftest.py"
            if not conftest_path.exists():
                conftest_content = '''"""
ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•é…ç½®
"""

import pytest
import os
import sys
from pathlib import Path


@pytest.fixture(scope="session")
def project_root():
    """é¡¹ç›®æ ¹ç›®å½•"""
    return Path(__file__).parent.parent.parent.parent


@pytest.fixture(autouse=True)
def setup_test_env(project_root):
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    # æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
    src_path = str(project_root / "src")
    if src_path not in sys.path:
        sys.path.insert(0, src_path)
'''
                
                with open(conftest_path, 'w', encoding='utf-8') as f:
                    f.write(conftest_content)
        
        self.logger.info("  âœ… ç»¼åˆæµ‹è¯•ç”¨ä¾‹å·²æ·»åŠ ")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="XWE DevBuddy ä¼˜åŒ–è¡¥ä¸")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¡¥ä¸å™¨
    patcher = XWEOptimizationPatcher(verbose=args.verbose)
    
    # åº”ç”¨ä¼˜åŒ–
    if patcher.apply_optimizations():
        patcher.logger.info("ğŸ‰ ä¼˜åŒ–åº”ç”¨æˆåŠŸ!")
        
        # è¾“å‡ºä½¿ç”¨è¯´æ˜
        print("\n" + "="*60)
        print("ğŸ“– ä¼˜åŒ–åŠŸèƒ½ä½¿ç”¨è¯´æ˜:")
        print("="*60)
        print("1. ä½¿ç”¨æ–°çš„ CLI å·¥å…·:")
        print("   python scripts/xwe_cli.py --verbose")
        print("   python scripts/xwe_cli.py --mock-llm --max-retries 5")
        print("")
        print("2. ç¯å¢ƒå˜é‡é…ç½®:")
        print("   export USE_MOCK_LLM=true      # å¯ç”¨ Mock æ¨¡å¼")
        print("   export XWE_MAX_LLM_RETRIES=5  # è®¾ç½®é‡è¯•æ¬¡æ•°")
        print("   export VERBOSE_LOG=true       # å¯ç”¨è¯¦ç»†æ—¥å¿—")
        print("")
        print("3. è¿è¡Œä¼˜åŒ–æµ‹è¯•:")
        print("   pytest tests/unit/optimizations/ -v")
        print("   pytest tests/e2e/optimizations/ -v")
        print("")
        print("4. åŠŸèƒ½ç‰¹æ€§:")
        print("   âœ… å¯é…ç½® API é‡è¯•æ¬¡æ•°")
        print("   âœ… Mock æ¨¡å¼è·³è¿‡ç½‘ç»œè°ƒç”¨")
        print("   âœ… Token é•¿åº¦è‡ªåŠ¨ä¿æŠ¤")
        print("   âœ… ä¼˜åŒ–çš„æ—¥å¿—çº§åˆ«æ§åˆ¶")
        print("   âœ… CLI è¯¦ç»†æ¨¡å¼æ”¯æŒ")
        print("="*60)
        
        return True
    else:
        patcher.logger.error("âŒ ä¼˜åŒ–åº”ç”¨å¤±è´¥")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
