#!/usr/bin/env python3
"""
æ™ºèƒ½é¡¹ç›®æ¸…ç†è„šæœ¬ - ç”¨äºæ¸…ç†ä»™ä¾ ä¸–ç•Œå¼•æ“é¡¹ç›®ä¸­çš„é‡å¤æ–‡ä»¶
"""

import difflib
import hashlib
import json
import os
import shutil
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple


class ProjectCleaner:
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.duplicate_groups = defaultdict(list)
        self.file_checksums = {}
        self.cleanup_report = {
            "scanned_files": 0,
            "duplicate_groups": 0,
            "files_to_delete": [],
            "space_to_free": 0,
            "warnings": [],
            "recommendations": [],
        }

    def calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5æ ¡éªŒå’Œ"""
        try:
            with open(file_path, "rb") as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            self.cleanup_report["warnings"].append(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return None

    def calculate_similarity(self, file1: Path, file2: Path) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶çš„ç›¸ä¼¼åº¦ï¼ˆä»…ç”¨äºæ–‡æœ¬æ–‡ä»¶ï¼‰"""
        try:
            with open(file1, "r", encoding="utf-8") as f1:
                content1 = f1.read()
            with open(file2, "r", encoding="utf-8") as f2:
                content2 = f2.read()

            # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
            similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
            return similarity
        except Exception:
            return 0.0

    def find_json_duplicates(self):
        """æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶å¹¶åˆ†æé‡å¤"""
        print("ğŸ” æ‰«æJSONæ–‡ä»¶...")
        json_files = []

        # æ”¶é›†æ‰€æœ‰JSONæ–‡ä»¶
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡gitç›®å½•å’Œnode_modules
            if ".git" in root or "node_modules" in root or "__pycache__" in root:
                continue

            for file in files:
                if file.endswith(".json"):
                    file_path = Path(root) / file
                    json_files.append(file_path)
                    self.cleanup_report["scanned_files"] += 1

        print(f"ğŸ“Š æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

        # åˆ†ææ–‡ä»¶å†…å®¹
        self._analyze_duplicate_content(json_files)

        # åˆ†æåµŒå¥—ç›®å½•é—®é¢˜
        self._analyze_nested_directories()

        # åˆ†æåŒåæ–‡ä»¶
        self._analyze_same_name_files(json_files)

    def _analyze_duplicate_content(self, files: List[Path]):
        """åˆ†ææ–‡ä»¶å†…å®¹çš„é‡å¤æ€§"""
        print("\nğŸ” åˆ†ææ–‡ä»¶å†…å®¹...")

        # æŒ‰æ–‡ä»¶ååˆ†ç»„
        name_groups = defaultdict(list)
        for file_path in files:
            base_name = file_path.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
            name_groups[base_name].append(file_path)

        # åˆ†ææ¯ç»„åŒåæ–‡ä»¶
        for base_name, file_list in name_groups.items():
            if len(file_list) > 1:
                print(f"\nğŸ“„ åˆ†æ '{base_name}' æ–‡ä»¶ç»„ ({len(file_list)} ä¸ªæ–‡ä»¶)...")

                # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„å¤§å°å’Œæ ¡éªŒå’Œ
                file_info = []
                for file_path in file_list:
                    size = file_path.stat().st_size
                    checksum = self.calculate_checksum(file_path)
                    mtime = file_path.stat().st_mtime

                    file_info.append(
                        {
                            "path": file_path,
                            "size": size,
                            "checksum": checksum,
                            "mtime": mtime,
                            "relative_path": file_path.relative_to(self.project_root),
                        }
                    )

                # æŒ‰æ ¡éªŒå’Œåˆ†ç»„
                checksum_groups = defaultdict(list)
                for info in file_info:
                    if info["checksum"]:
                        checksum_groups[info["checksum"]].append(info)

                # è®°å½•å®Œå…¨ç›¸åŒçš„æ–‡ä»¶
                for checksum, group in checksum_groups.items():
                    if len(group) > 1:
                        self.duplicate_groups[f"{base_name}_{checksum[:8]}"] = group
                        self.cleanup_report["duplicate_groups"] += 1

                # å¯¹äºå†…å®¹ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
                if len(checksum_groups) > 1:
                    self._analyze_similar_files(base_name, file_info)

    def _analyze_similar_files(self, base_name: str, file_info: List[Dict]):
        """åˆ†æç›¸ä¼¼ä½†ä¸å®Œå…¨ç›¸åŒçš„æ–‡ä»¶"""
        print(f"  ğŸ“Š åˆ†æ '{base_name}' çš„ä¸åŒç‰ˆæœ¬...")

        # æ‰¾å‡ºæœ€å¤§å’Œæœ€æ–°çš„æ–‡ä»¶
        largest = max(file_info, key=lambda x: x["size"])
        newest = max(file_info, key=lambda x: x["mtime"])

        recommendation = {
            "file_group": base_name,
            "files": [str(f["relative_path"]) for f in file_info],
            "suggested_keep": str(newest["relative_path"]),
            "reason": "æœ€æ–°ä¿®æ”¹æ—¶é—´",
            "largest_file": str(largest["relative_path"]),
            "size_difference": f"{max(f['size'] for f in file_info) - min(f['size'] for f in file_info)} bytes",
        }

        self.cleanup_report["recommendations"].append(recommendation)

    def _analyze_nested_directories(self):
        """åˆ†æåµŒå¥—çš„é‡å¤ç›®å½•ç»“æ„"""
        print("\nğŸ” æ£€æŸ¥åµŒå¥—ç›®å½•é—®é¢˜...")

        # æŸ¥æ‰¾å¯ç–‘çš„åµŒå¥—æ¨¡å¼
        suspicious_patterns = [
            "data/restructured",
            "xwe/data/restructured",
            "xwe/data/refactored",
        ]

        for pattern in suspicious_patterns:
            pattern_path = self.project_root / pattern
            if pattern_path.exists():
                self.cleanup_report["warnings"].append(f"å‘ç°å¯ç–‘çš„åµŒå¥—ç›®å½•: {pattern}")

    def _analyze_same_name_files(self, files: List[Path]):
        """åˆ†æåŒåæ–‡ä»¶çš„åˆ†å¸ƒ"""
        print("\nğŸ” åˆ†æåŒåæ–‡ä»¶åˆ†å¸ƒ...")

        name_locations = defaultdict(list)
        for file_path in files:
            name_locations[file_path.name].append(
                str(file_path.relative_to(self.project_root))
            )

        # è®°å½•åˆ†æ•£åœ¨å¤šå¤„çš„åŒåæ–‡ä»¶
        for filename, locations in name_locations.items():
            if len(locations) > 2:  # è¶…è¿‡2ä¸ªä½ç½®
                self.cleanup_report["warnings"].append(
                    f"æ–‡ä»¶ '{filename}' åˆ†æ•£åœ¨ {len(locations)} ä¸ªä½ç½®"
                )

    def generate_cleanup_plan(self):
        """ç”Ÿæˆæ¸…ç†è®¡åˆ’"""
        print("\nğŸ“‹ ç”Ÿæˆæ¸…ç†è®¡åˆ’...")

        cleanup_plan = {
            "delete_files": [],
            "merge_suggestions": [],
            "restructure_suggestions": [],
        }

        # 1. æ ‡è®°å®Œå…¨é‡å¤çš„æ–‡ä»¶
        for group_name, duplicates in self.duplicate_groups.items():
            if len(duplicates) > 1:
                # ä¿ç•™æœ€æ–°çš„æ–‡ä»¶
                keep = max(duplicates, key=lambda x: x["mtime"])
                for dup in duplicates:
                    if dup != keep:
                        cleanup_plan["delete_files"].append(
                            {
                                "file": str(dup["relative_path"]),
                                "reason": f"ä¸ {keep['relative_path']} å®Œå…¨ç›¸åŒ",
                                "size": dup["size"],
                            }
                        )
                        self.cleanup_report["space_to_free"] += dup["size"]

        # 2. å»ºè®®åˆå¹¶ç›¸ä¼¼æ–‡ä»¶
        cleanup_plan["merge_suggestions"] = [
            {
                "description": "åˆå¹¶å¤šä¸ªrestructuredç›®å½•",
                "affected_dirs": [
                    "data/restructured",
                    "xwe/data/restructured",
                    "xwe/data/refactored",
                ],
                "suggested_location": "data/game_configs",
            }
        ]

        # 3. é‡æ„å»ºè®®
        cleanup_plan["restructure_suggestions"] = [
            "å°†æ‰€æœ‰æ¸¸æˆé…ç½®JSONæ–‡ä»¶ç»Ÿä¸€æ”¾åœ¨ data/game_configs ç›®å½•",
            "å°†ä¸åŒç‰ˆæœ¬çš„é…ç½®æ–‡ä»¶ï¼ˆå¦‚combat_system_v2.jsonï¼‰æ”¹ä¸ºä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿç®¡ç†",
            "åˆ é™¤ç©ºçš„JSONæ–‡ä»¶å’Œæµ‹è¯•æ–‡ä»¶",
            "ä¸ºé…ç½®æ–‡ä»¶å»ºç«‹æ¸…æ™°çš„å‘½åè§„èŒƒ",
        ]

        return cleanup_plan

    def execute_cleanup(self, cleanup_plan: Dict, interactive: bool = True):
        """æ‰§è¡Œæ¸…ç†è®¡åˆ’"""
        print("\nğŸ§¹ å‡†å¤‡æ‰§è¡Œæ¸…ç†...")

        deleted_files = []
        skipped_files = []

        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = (
            self.project_root / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )

        if interactive:
            print(f"\nâš ï¸  å°†è¦åˆ é™¤ {len(cleanup_plan['delete_files'])} ä¸ªæ–‡ä»¶")
            print(f"ğŸ’¾ å¤‡ä»½å°†ä¿å­˜åœ¨: {backup_dir}")

            response = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
            if response.lower() != "y":
                print("âŒ æ¸…ç†å·²å–æ¶ˆ")
                return

        backup_dir.mkdir(exist_ok=True)

        # æ‰§è¡Œæ–‡ä»¶åˆ é™¤
        for file_info in cleanup_plan["delete_files"]:
            file_path = self.project_root / file_info["file"]

            try:
                if interactive:
                    print(f"\nğŸ“„ æ–‡ä»¶: {file_info['file']}")
                    print(f"   åŸå› : {file_info['reason']}")
                    print(f"   å¤§å°: {file_info['size']} bytes")
                    action = input("   åˆ é™¤æ­¤æ–‡ä»¶ï¼Ÿ(y/n/sè·³è¿‡æ‰€æœ‰): ")

                    if action.lower() == "s":
                        interactive = False
                    elif action.lower() != "y":
                        skipped_files.append(file_info["file"])
                        continue

                # å¤‡ä»½æ–‡ä»¶
                backup_path = backup_dir / file_info["file"]
                backup_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(file_path, backup_path)

                # åˆ é™¤æ–‡ä»¶
                file_path.unlink()
                deleted_files.append(file_info["file"])
                print(f"   âœ… å·²åˆ é™¤")

            except Exception as e:
                print(f"   âŒ é”™è¯¯: {e}")
                self.cleanup_report["warnings"].append(f"æ— æ³•åˆ é™¤ {file_info['file']}: {e}")

        # ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
        self._generate_final_report(deleted_files, skipped_files, cleanup_plan)

    def _generate_final_report(
        self, deleted_files: List[str], skipped_files: List[str], cleanup_plan: Dict
    ):
        """ç”Ÿæˆæœ€ç»ˆæ¸…ç†æŠ¥å‘Š"""
        report_path = (
            self.project_root
            / f"cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        )

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("# é¡¹ç›®æ¸…ç†æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## æ¸…ç†æ‘˜è¦\n\n")
            f.write(f"- æ‰«ææ–‡ä»¶æ•°: {self.cleanup_report['scanned_files']}\n")
            f.write(f"- å‘ç°é‡å¤ç»„: {self.cleanup_report['duplicate_groups']}\n")
            f.write(f"- åˆ é™¤æ–‡ä»¶æ•°: {len(deleted_files)}\n")
            f.write(f"- è·³è¿‡æ–‡ä»¶æ•°: {len(skipped_files)}\n")
            f.write(f"- é‡Šæ”¾ç©ºé—´: {self.cleanup_report['space_to_free'] / 1024:.2f} KB\n\n")

            if deleted_files:
                f.write("## å·²åˆ é™¤æ–‡ä»¶\n\n")
                for file in deleted_files:
                    f.write(f"- {file}\n")
                f.write("\n")

            if skipped_files:
                f.write("## è·³è¿‡çš„æ–‡ä»¶\n\n")
                for file in skipped_files:
                    f.write(f"- {file}\n")
                f.write("\n")

            if self.cleanup_report["warnings"]:
                f.write("## è­¦å‘Š\n\n")
                for warning in self.cleanup_report["warnings"]:
                    f.write(f"- âš ï¸  {warning}\n")
                f.write("\n")

            if self.cleanup_report["recommendations"]:
                f.write("## æ–‡ä»¶åˆå¹¶å»ºè®®\n\n")
                for rec in self.cleanup_report["recommendations"]:
                    f.write(f"### {rec['file_group']}\n")
                    f.write(f"- å»ºè®®ä¿ç•™: `{rec['suggested_keep']}`\n")
                    f.write(f"- åŸå› : {rec['reason']}\n")
                    f.write(f"- æ¶‰åŠæ–‡ä»¶:\n")
                    for file in rec["files"]:
                        f.write(f"  - {file}\n")
                    f.write("\n")

            if cleanup_plan["restructure_suggestions"]:
                f.write("## é¡¹ç›®é‡æ„å»ºè®®\n\n")
                for suggestion in cleanup_plan["restructure_suggestions"]:
                    f.write(f"- {suggestion}\n")
                f.write("\n")

            f.write("## ä¸‹ä¸€æ­¥å»ºè®®\n\n")
            f.write("1. å®¡æŸ¥åˆå¹¶å»ºè®®ï¼Œæ‰‹åŠ¨åˆå¹¶ç›¸ä¼¼ä½†ä¸å®Œå…¨ç›¸åŒçš„é…ç½®æ–‡ä»¶\n")
            f.write("2. å®æ–½é¡¹ç›®é‡æ„å»ºè®®ï¼Œå»ºç«‹æ›´æ¸…æ™°çš„ç›®å½•ç»“æ„\n")
            f.write("3. ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿç®¡ç†é…ç½®æ–‡ä»¶çš„ä¸åŒç‰ˆæœ¬\n")
            f.write("4. å»ºç«‹æ–‡ä»¶å‘½åå’Œç»„ç»‡è§„èŒƒï¼Œé¿å…æœªæ¥å‡ºç°ç±»ä¼¼é—®é¢˜\n")

        print(f"\nğŸ“„ æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

    def run(self, interactive: bool = True):
        """è¿è¡Œæ¸…ç†æµç¨‹"""
        print("ğŸš€ å¼€å§‹é¡¹ç›®æ¸…ç†åˆ†æ...\n")

        # 1. æŸ¥æ‰¾é‡å¤æ–‡ä»¶
        self.find_json_duplicates()

        # 2. ç”Ÿæˆæ¸…ç†è®¡åˆ’
        cleanup_plan = self.generate_cleanup_plan()

        # 3. æ˜¾ç¤ºæ¸…ç†è®¡åˆ’æ‘˜è¦
        print("\nğŸ“Š æ¸…ç†è®¡åˆ’æ‘˜è¦:")
        print(f"  - å°†åˆ é™¤ {len(cleanup_plan['delete_files'])} ä¸ªé‡å¤æ–‡ä»¶")
        print(f"  - å¯é‡Šæ”¾ç©ºé—´: {self.cleanup_report['space_to_free'] / 1024:.2f} KB")
        print(f"  - å‘ç° {len(self.cleanup_report['warnings'])} ä¸ªæ½œåœ¨é—®é¢˜")

        # 4. æ‰§è¡Œæ¸…ç†
        if cleanup_plan["delete_files"]:
            self.execute_cleanup(cleanup_plan, interactive)
        else:
            print("\nâœ¨ æœªå‘ç°éœ€è¦åˆ é™¤çš„å®Œå…¨é‡å¤æ–‡ä»¶")

            # ä»ç„¶ç”ŸæˆæŠ¥å‘Šï¼ŒåŒ…å«å»ºè®®
            self._generate_final_report([], [], cleanup_plan)

        print("\nâœ… æ¸…ç†å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = "/path/to/xianxia_world_engine"

    print("=" * 60)
    print("ğŸ§¹ ä»™ä¾ ä¸–ç•Œå¼•æ“é¡¹ç›®æ¸…ç†å·¥å…·")
    print("=" * 60)

    # åˆ›å»ºæ¸…ç†å™¨å®ä¾‹
    cleaner = ProjectCleaner(project_root)

    # è¿è¡Œæ¸…ç†
    cleaner.run(interactive=True)

    print("\nğŸ’¡ æç¤º: æ‰€æœ‰è¢«åˆ é™¤çš„æ–‡ä»¶éƒ½å·²å¤‡ä»½ï¼Œå¦‚éœ€æ¢å¤è¯·æŸ¥çœ‹backupç›®å½•")


if __name__ == "__main__":
    main()
