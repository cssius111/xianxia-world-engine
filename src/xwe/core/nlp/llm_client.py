"""
DeepSeek API å®¢æˆ·ç«¯ - ä¼˜åŒ–ç‰ˆæœ¬
å¤„ç†ä¸ DeepSeek API çš„é€šä¿¡ï¼Œæ”¯æŒå¯é…ç½®é‡è¯•å’Œ Mock æ¨¡å¼
"""

import asyncio
import functools
import json
import logging
import os
import re
from concurrent.futures import ThreadPoolExecutor
from time import sleep, time
from typing import Any, Dict, Optional, Union

try:
    import requests
except ImportError:  # pragma: no cover - ç¯å¢ƒç¼ºå°‘ requests æ—¶ä½¿ç”¨å ä½å¯¹è±¡
    requests = None

# å¯¼å…¥ Prometheus æŒ‡æ ‡æ”¶é›†å™¨
try:
    from ...metrics.prometheus_metrics import get_metrics_collector
    PROMETHEUS_ENABLED = True
except ImportError:
    PROMETHEUS_ENABLED = False

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ backoffï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç®€å•çš„é‡è¯•è£…é¥°å™¨
try:
    import backoff
    HAS_BACKOFF = True
except ImportError:
    HAS_BACKOFF = False
    logger.warning("backoff æ¨¡å—æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•é‡è¯•æœºåˆ¶")


def simple_retry(max_tries=3, delay=1.0):
    """ç®€å•çš„é‡è¯•è£…é¥°å™¨ï¼ˆå½“ backoff ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_tries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_tries - 1:
                        sleep(delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    else:
                        raise
            raise last_exception
        return wrapper
    return decorator


class LLMClient:
    """
    DeepSeek API å®¢æˆ·ç«¯ - ä¼˜åŒ–ç‰ˆæœ¬
    
    æ–°å¢åŠŸèƒ½ï¼š
    - å¯é…ç½®é‡è¯•æ¬¡æ•° (XWE_MAX_LLM_RETRIES)
    - Mock æ¨¡å¼æ”¯æŒ (USE_MOCK_LLM=true)
    - æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_url: str = "https://api.deepseek.com/v1/chat/completions",
        model_name: str = "deepseek-chat",
        timeout: int = 30,
        debug: bool = False,
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            api_url: APIç«¯ç‚¹URL
            model_name: æ¨¡å‹åç§°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ Mock æ¨¡å¼
        self.use_mock = os.getenv("USE_MOCK_LLM", "false").lower() == "true"
        
        if self.use_mock:
            logger.info("ğŸ­ LLM Mock æ¨¡å¼å·²å¯ç”¨ï¼Œå°†è·³è¿‡ç½‘ç»œè°ƒç”¨")
            self.api_key = "mock_key"
        else:
            self.api_key = api_key or os.environ.get("DEEPSEEK_API_KEY")
            if not self.api_key:
                raise ValueError("DEEPSEEK_API_KEY not found in environment variables")

        self.api_url = api_url
        self.model_name = model_name
        self.timeout = timeout
        self.debug = debug
        
        # å¯é…ç½®çš„é‡è¯•æ¬¡æ•°
        self.max_retries = int(os.getenv("XWE_MAX_LLM_RETRIES", "3"))

        # è¯·æ±‚å¤´
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        # åˆå§‹åŒ–çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼ˆç”¨äºå¼‚æ­¥åŒ…è£…ï¼‰
        self._executor = ThreadPoolExecutor(
            max_workers=int(os.getenv("LLM_ASYNC_WORKERS", "5")),
            thread_name_prefix="llm_async_"
        )
        self._executor_initialized = True
        logger.info(f"LLMClient çº¿ç¨‹æ± åˆå§‹åŒ–å®Œæˆï¼Œå·¥ä½œçº¿ç¨‹æ•°: {self._executor._max_workers}")
        
        # Prometheus æŒ‡æ ‡æ”¶é›†å™¨
        self.metrics_collector = None
        if PROMETHEUS_ENABLED:
            self.metrics_collector = get_metrics_collector()
            self.prometheus_enabled = os.getenv('ENABLE_PROMETHEUS', 'true').lower() == 'true'
            # æ›´æ–°çº¿ç¨‹æ± å¤§å°æŒ‡æ ‡
            if self.prometheus_enabled:
                self.metrics_collector.update_async_metrics(
                    thread_pool_size=self._executor._max_workers
                )
        else:
            self.prometheus_enabled = False
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†çº¿ç¨‹æ± èµ„æº"""
        if hasattr(self, '_executor_initialized') and self._executor_initialized:
            try:
                self._executor.shutdown(wait=True, cancel_futures=True)
                self._executor_initialized = False
                logger.info("LLMClient çº¿ç¨‹æ± å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­çº¿ç¨‹æ± æ—¶å‡ºé”™: {e}")

    def _make_request_with_retry(self, payload: Dict) -> Dict:
        """å‘é€è¯·æ±‚çš„åŒ…è£…æ–¹æ³•"""
        if self.use_mock:
            return self._mock_response(payload)
        
        if HAS_BACKOFF:
            # ä½¿ç”¨ backoff åº“çš„é«˜çº§é‡è¯•æœºåˆ¶
            @backoff.on_exception(
                backoff.expo,
                (requests.exceptions.RequestException, requests.exceptions.Timeout),
                max_tries=self.max_retries,
                max_time=60,
            )
            def _request():
                return self._make_request(payload)
            return _request()
        else:
            # ä½¿ç”¨ç®€å•é‡è¯•æœºåˆ¶
            @simple_retry(max_tries=self.max_retries, delay=1.0)
            def _request():
                return self._make_request(payload)
            return _request()

    def _mock_response(self, payload: Dict) -> Dict:
        """Mock å“åº”ç”Ÿæˆå™¨"""
        # ä» payload ä¸­æå–ç”¨æˆ·æ¶ˆæ¯
        messages = payload.get("messages", [])
        user_message = ""
        actual_user_input = ""
        
        for msg in messages:
            if msg.get("role") == "user":
                user_message = msg.get("content", "")
                # ä» prompt ä¸­æå–å®é™…çš„ç”¨æˆ·è¾“å…¥
                # æŸ¥æ‰¾æœ€åä¸€ä¸ª "è¾“å…¥:" æ ‡è®°
                last_input_idx = user_message.rfind('è¾“å…¥:')
                if last_input_idx != -1:
                    # æ‰¾åˆ°åé¢çš„å¼•å·å†…å®¹
                    start_quote = user_message.find('"', last_input_idx)
                    if start_quote != -1:
                        end_quote = user_message.find('"', start_quote + 1)
                        if end_quote != -1:
                            actual_user_input = user_message[start_quote + 1:end_quote]
                break
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œä½¿ç”¨æ•´ä¸ªç”¨æˆ·æ¶ˆæ¯
        if not actual_user_input:
            actual_user_input = user_message
        
        # ç®€å•çš„æœ¬åœ°è§£æé€»è¾‘
        mock_responses = {
            "æ¢ç´¢": {
                "normalized_command": "æ¢ç´¢",
                "intent": "action",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šæ¢ç´¢å‘½ä»¤"
            },
            "ä¿®ç‚¼": {
                "normalized_command": "ä¿®ç‚¼",
                "intent": "train",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šä¿®ç‚¼å‘½ä»¤"
            },
            "èƒŒåŒ…": {
                "normalized_command": "æ‰“å¼€èƒŒåŒ…",
                "intent": "check",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šèƒŒåŒ…å‘½ä»¤"
            },
            "çŠ¶æ€": {
                "normalized_command": "æŸ¥çœ‹çŠ¶æ€",
                "intent": "check",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šçŠ¶æ€å‘½ä»¤"
            }
        }
        
        # å°è¯•åŒ¹é…ç”¨æˆ·è¾“å…¥
        # å…ˆå°è¯•å®Œæ•´åŒ¹é…
        exact_matches = {
            "æ¢ç´¢å‘¨å›´ç¯å¢ƒ": {
                "normalized_command": "æ¢ç´¢",
                "intent": "action",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šæ¢ç´¢å‘½ä»¤"
            },
            "ä¿®ç‚¼æå‡å®åŠ›": {
                "normalized_command": "ä¿®ç‚¼",
                "intent": "train",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šä¿®ç‚¼å‘½ä»¤"
            },
            "æŸ¥çœ‹å½“å‰çŠ¶æ€": {
                "normalized_command": "æŸ¥çœ‹çŠ¶æ€",
                "intent": "check",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šçŠ¶æ€å‘½ä»¤"
            },
            "æ‰“å¼€èƒŒåŒ…çœ‹çœ‹": {
                "normalized_command": "æ‰“å¼€èƒŒåŒ…",
                "intent": "check",
                "args": {},
                "explanation": "Mockæ¨¡å¼ï¼šèƒŒåŒ…å‘½ä»¤"
            }
        }
        
        # å…ˆå°è¯•å®Œæ•´åŒ¹é…
        if actual_user_input in exact_matches:
            response_copy = exact_matches[actual_user_input].copy()
            response_copy["raw"] = actual_user_input
            return {
                "choices": [
                    {
                        "message": {
                            "content": json.dumps(response_copy, ensure_ascii=False)
                        }
                    }
                ]
            }
        
        # å¦‚æœæ²¡æœ‰å®Œæ•´åŒ¹é…ï¼Œå°è¯•å…³é”®è¯åŒ¹é…
        for keyword, response in mock_responses.items():
            if keyword in actual_user_input.lower():
                response_copy = response.copy()
                response_copy["raw"] = actual_user_input
                return {
                    "choices": [
                        {
                            "message": {
                                "content": json.dumps(response_copy, ensure_ascii=False)
                            }
                        }
                    ]
                }
        
        # é»˜è®¤å“åº”
        default_response = {
            "raw": actual_user_input,
            "normalized_command": "æœªçŸ¥",
            "intent": "unknown",
            "args": {},
            "explanation": "Mockæ¨¡å¼ï¼šæœªçŸ¥å‘½ä»¤"
        }
        
        return {
            "choices": [
                {
                    "message": {
                        "content": json.dumps(default_response, ensure_ascii=False)
                    }
                }
            ]
        }

    def _make_request(self, payload: Dict) -> Dict:
        """
        å‘é€è¯·æ±‚åˆ° DeepSeek API

        Args:
            payload: è¯·æ±‚è½½è·

        Returns:
            APIå“åº”
        """
        if requests is None:
            raise ImportError("The 'requests' package is required for network operations")

        start_time = time()
        try:
            response = requests.post(
                self.api_url, headers=self.headers, json=payload, timeout=self.timeout
            )
            response.raise_for_status()
            
            # è®°å½• API è°ƒç”¨å»¶è¿Ÿ
            if self.prometheus_enabled and self.metrics_collector:
                duration = time() - start_time
                self.metrics_collector.record_api_call(
                    api_name="deepseek",
                    endpoint=self.api_url,
                    duration=duration
                )
            
            return response.json()

        except requests.exceptions.Timeout:
            logger.warning(f"Request to DeepSeek API timed out after {self.timeout}s")
            raise

        except requests.exceptions.RequestException as e:
            logger.warning(f"Request to DeepSeek API failed: {e}")
            if hasattr(e, "response") and e.response is not None:
                logger.warning(f"Response status: {e.response.status_code}")
                logger.warning(f"Response body: {e.response.text}")
            raise

    def chat(
        self,
        prompt: str,
        temperature: float = 0.0,
        max_tokens: int = 256,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        å‘é€èŠå¤©è¯·æ±‚

        Args:
            prompt: ç”¨æˆ·æç¤º
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰ï¼Œ0è¡¨ç¤ºæ›´ç¡®å®šçš„è¾“å‡º
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰

        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": prompt})

        # æ„å»ºè¯·æ±‚è½½è·
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        # è®°å½•è¯·æ±‚ï¼ˆä»…åœ¨é Mock æ¨¡å¼ä¸‹ï¼‰
        if not self.use_mock:
            logger.debug(
                f"Sending request to DeepSeek API: {json.dumps(payload, ensure_ascii=False)[:200]}..."
            )

        try:
            # åœ¨ Mock æ¨¡å¼ä¸‹æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿï¼Œä½¿åŒæ­¥è°ƒç”¨æ›´æ¥è¿‘çœŸå®æƒ…å†µ
            if self.use_mock:
                sleep(0.1)
            # å‘é€è¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰
            start_time = time()
            response = self._make_request_with_retry(payload)
            elapsed = time() - start_time

            if self.use_mock:
                logger.debug(f"Mock response generated in {elapsed:.2f}s")
            else:
                logger.debug(f"DeepSeek API response received in {elapsed:.2f}s")
            
            if self.debug:
                logger.debug(
                    f"DeepSeek full response: {json.dumps(response, ensure_ascii=False)}"
                )

            # æå–å“åº”æ–‡æœ¬
            if "choices" in response and len(response["choices"]) > 0:
                content = response["choices"][0].get("message", {}).get("content", "")
                return content
            else:
                logger.error(f"Unexpected response format: {response}")
                return ""

        except Exception as e:
            logger.error(f"Error calling DeepSeek API: {e}")
            raise
    
    async def chat_async(
        self,
        prompt: str,
        temperature: float = 0.0,
        max_tokens: int = 256,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        å¼‚æ­¥å‘é€èŠå¤©è¯·æ±‚ï¼ˆçº¿ç¨‹æ± åŒ…è£…ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰ï¼Œ0è¡¨ç¤ºæ›´ç¡®å®šçš„è¾“å‡º
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
            
        Example:
            ```python
            import asyncio
            
            async def main():
                client = LLMClient()
                response = await client.chat_async("ä½ å¥½")
                print(response)
                
            asyncio.run(main())
            ```
        """
        if not self._executor_initialized:
            raise RuntimeError("LLMClient çº¿ç¨‹æ± å·²å…³é—­")
        
        # åœ¨ Mock æ¨¡å¼ä¸‹é¿å…çº¿ç¨‹æ± å¼€é”€
        if self.use_mock:
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            response = self._mock_response(payload)
            content = response.get("choices", [{}])[0].get("message", {}).get("content", "")
            return content

        loop = asyncio.get_event_loop()

        func = functools.partial(
            self.chat,
            prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt
        )

        try:
            result = await loop.run_in_executor(self._executor, func)
            return result
        except Exception as e:
            logger.error(f"Async chat error: {e}")
            raise

    def chat_with_context(
        self, messages: list, temperature: float = 0.7, max_tokens: int = 256
    ) -> str:
        """
        å¸¦ä¸Šä¸‹æ–‡çš„èŠå¤©

        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user/assistant/system", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        try:
            response = self._make_request_with_retry(payload)
            if self.debug:
                logger.debug(
                    f"DeepSeek full response: {json.dumps(response, ensure_ascii=False)}"
                )

            if "choices" in response and len(response["choices"]) > 0:
                content = response["choices"][0].get("message", {}).get("content", "")
                return content
            else:
                return ""

        except Exception as e:
            logger.error(f"Error in chat_with_context: {e}")
            raise
    
    async def chat_with_context_async(
        self, messages: list, temperature: float = 0.7, max_tokens: int = 256
    ) -> str:
        """
        å¼‚æ­¥å¸¦ä¸Šä¸‹æ–‡çš„èŠå¤©ï¼ˆçº¿ç¨‹æ± åŒ…è£…ï¼‰
        
        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user/assistant/system", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
            
        Example:
            ```python
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ¸¸æˆåŠ©æ‰‹"},
                {"role": "user", "content": "å¦‚ä½•æå‡å¢ƒç•Œ?"},
                {"role": "assistant", "content": "éœ€è¦ä¸æ–­ä¿®ç‚¼..."},
                {"role": "user", "content": "å…·ä½“æ€ä¹ˆæ“ä½œ?"}
            ]
            response = await client.chat_with_context_async(messages)
            ```
        """
        if not self._executor_initialized:
            raise RuntimeError("LLMClient çº¿ç¨‹æ± å·²å…³é—­")
        
        loop = asyncio.get_event_loop()
        
        # ä½¿ç”¨ functools.partial åˆ›å»ºå¸¦å‚æ•°çš„å‡½æ•°
        func = functools.partial(
            self.chat_with_context,
            messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
        try:
            result = await loop.run_in_executor(self._executor, func)
            return result
        except Exception as e:
            logger.error(f"Async chat_with_context error: {e}")
            raise

    def get_embeddings(self, text: str) -> Optional[list]:
        """
        è·å–æ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆå¦‚æœAPIæ”¯æŒï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            åµŒå…¥å‘é‡
        """
        # æ³¨æ„ï¼šDeepSeek APIå¯èƒ½ä¸æ”¯æŒåµŒå…¥åŠŸèƒ½
        # è¿™é‡Œåªæ˜¯é¢„ç•™æ¥å£
        logger.warning("Embeddings API not implemented for DeepSeek")
        return None


# ä¿æŒå‘åå…¼å®¹
class DeepSeek:
    """å‘åå…¼å®¹çš„DeepSeekç±»"""

    def __init__(self, api_key: str, model: str = "deepseek-chat"):
        self.client = LLMClient(api_key=api_key, model_name=model)

    def chat(self, prompt: str) -> Dict[str, Any]:
        """å…¼å®¹æ—§æ¥å£"""
        try:
            text = self.client.chat(prompt)
            return {"text": text}
        except Exception as e:
            logger.error(f"Error in legacy chat method: {e}")
            return {"text": ""}
    
    async def chat_async(self, prompt: str) -> Dict[str, Any]:
        """å¼‚æ­¥å…¼å®¹æ—§æ¥å£"""
        try:
            text = await self.client.chat_async(prompt)
            return {"text": text}
        except Exception as e:
            logger.error(f"Error in legacy async chat method: {e}")
            return {"text": ""}
