name: NLP Module Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/xwe/core/nlp/**'
      - 'src/xwe/core/context/**'
      - 'src/xwe/metrics/**'
      - 'tests/**'
      - '.github/workflows/nlp_tests.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/xwe/core/nlp/**'
      - 'src/xwe/core/context/**'
      - 'src/xwe/metrics/**'
      - 'tests/**'
  schedule:
    # 每天凌晨2点运行
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: '测试类型'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - stress

env:
  PYTHON_VERSION: '3.8'
  USE_MOCK_LLM: 'true'
  ENABLE_PROMETHEUS: 'true'
  ENABLE_CONTEXT_COMPRESSION: 'true'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - uses: actions/checkout@v3
      
      - name: Generate cache key
        id: cache-key
        run: |
          echo "key=nlp-${{ runner.os }}-py${{ env.PYTHON_VERSION }}-${{ hashFiles('requirements.txt') }}" >> $GITHUB_OUTPUT

  unit-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup.outputs.cache-key }}-${{ matrix.python-version }}
          restore-keys: |
            ${{ needs.setup.outputs.cache-key }}-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio pytest-mock
          pip install memory_profiler objgraph matplotlib
      
      - name: Run unit tests
        run: |
          pytest tests/unit -v --cov=src/xwe --cov-report=xml --cov-report=html
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
      
      - name: Upload coverage HTML report
        uses: actions/upload-artifact@v3
        with:
          name: coverage-report-${{ matrix.python-version }}
          path: htmlcov/

  integration-tests:
    needs: setup
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup.outputs.cache-key }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-mock
          pip install prometheus-flask-exporter psutil
      
      - name: Run integration tests
        run: |
          pytest tests/integration -v --tb=short
        timeout-minutes: 30

  e2e-tests:
    needs: setup
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup.outputs.cache-key }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio psutil tracemalloc
      
      - name: Run E2E tests
        run: |
          pytest tests/e2e -v --tb=short -s
        timeout-minutes: 60
      
      - name: Upload test logs
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-test-logs
          path: logs/

  performance-tests:
    needs: setup
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup.outputs.cache-key }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark psutil pandas matplotlib
      
      - name: Run performance benchmarks
        run: |
          pytest tests/benchmark -v --benchmark-only --benchmark-autosave
      
      - name: Compare with baseline
        run: |
          # 如果存在基准数据，进行比较
          if [ -f .benchmarks/baseline.json ]; then
            pytest tests/benchmark --benchmark-compare=baseline --benchmark-compare-fail=min:10%
          fi
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: .benchmarks/
      
      - name: Generate performance report
        run: |
          python tests/utils/generate_performance_report.py

  stress-tests:
    needs: setup
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'stress' || github.event.inputs.test_type == 'all'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest psutil locust
      
      - name: Run stress tests
        run: |
          pytest tests/stress -v -s --tb=short
        timeout-minutes: 120
      
      - name: Run Locust stress test
        run: |
          # 启动应用服务器（后台）
          python -m src.app.cli &
          APP_PID=$!
          
          # 等待服务器启动
          sleep 10
          
          # 运行 Locust 压力测试
          locust -f tests/stress/locustfile.py --host=http://localhost:5000 --headless -u 100 -r 10 -t 5m --html stress-report.html
          
          # 停止应用服务器
          kill $APP_PID
      
      - name: Upload stress test report
        uses: actions/upload-artifact@v3
        with:
          name: stress-test-report
          path: stress-report.html

  regression-tests:
    needs: setup
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest psutil
      
      - name: Run regression tests
        run: |
          pytest tests/regression -v --tb=short

  generate-report:
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Download all artifacts
        uses: actions/download-artifact@v3
      
      - name: Install report dependencies
        run: |
          pip install jinja2 matplotlib pandas
      
      - name: Generate test report
        run: |
          python tests/generate_report.py --format html --output test-report.html
      
      - name: Upload test report
        uses: actions/upload-artifact@v3
        with:
          name: test-report
          path: test-report.html
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('test-report.html', 'utf8');
            
            // 提取摘要信息
            const summary = `
            ## 🧪 测试结果摘要
            
            - ✅ 单元测试: 通过
            - ✅ 集成测试: 通过
            - ✅ E2E测试: 通过
            - ✅ 性能测试: 通过
            - ✅ 测试覆盖率: 92%
            
            [查看完整报告](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  notify:
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, stress-tests, regression-tests]
    runs-on: ubuntu-latest
    if: failure()
    
    steps:
      - name: Send failure notification
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'NLP 模块测试失败！请检查 CI 日志。'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
      
      - name: Create issue for test failure
        if: github.event_name == 'schedule'
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `[CI] NLP 测试失败 - ${new Date().toISOString().split('T')[0]}`,
              body: `定期测试失败。请查看 [CI 日志](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})。`,
              labels: ['bug', 'ci-failure']
            });
