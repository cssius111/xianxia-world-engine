"""
ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶
æµ‹è¯•å®Œæ•´çš„ç”¨æˆ·åœºæ™¯å’Œç³»ç»Ÿé›†æˆ
"""

import pytest
import asyncio
import time
import json
import os
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
import tracemalloc
import gc

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT / "src"))

# è®¾ç½®æµ‹è¯•ç¯å¢ƒ
os.environ['USE_MOCK_LLM'] = 'true'
os.environ['ENABLE_PROMETHEUS'] = 'true'
os.environ['ENABLE_CONTEXT_COMPRESSION'] = 'true'


class TestNLPEndToEnd:
    """NLP æ¨¡å—ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    @pytest.fixture
    def app(self):
        """åˆ›å»ºæµ‹è¯•åº”ç”¨"""
        from app import create_app
        app = create_app()
        app.config['TESTING'] = True
        return app
    
    @pytest.fixture
    def client(self, app):
        """åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯"""
        return app.test_client()
    
    @pytest.fixture
    def nlp_processor(self):
        """åˆ›å»º NLP å¤„ç†å™¨"""
        from xwe.core.nlp.nlp_processor import NLPProcessor
        return NLPProcessor(use_context_compression=True)
    
    def test_complete_user_journey(self, client, nlp_processor):
        """æµ‹è¯•å®Œæ•´çš„ç”¨æˆ·æ—…ç¨‹"""
        # 1. ç”¨æˆ·æ³¨å†Œ/ç™»å½•
        response = client.post('/api/auth/login', json={
            'username': 'test_user',
            'password': 'test_pass'
        })
        assert response.status_code in [200, 404]  # å¯èƒ½éœ€è¦æ³¨å†Œ
        
        # 2. åˆ›å»ºæ¸¸æˆä¼šè¯
        response = client.post('/api/game/start', json={
            'player_name': 'æµ‹è¯•é“å‹',
            'difficulty': 'normal'
        })
        assert response.status_code in [200, 201]
        
        # 3. æ‰§è¡Œä¸€ç³»åˆ—æ¸¸æˆå‘½ä»¤
        test_commands = [
            "æŸ¥çœ‹æˆ‘çš„çŠ¶æ€",
            "æ¢ç´¢å‘¨å›´ç¯å¢ƒ",
            "æ‹¾å–çµçŸ³",
            "å¼€å§‹ä¿®ç‚¼",
            "æŸ¥çœ‹èƒŒåŒ…",
            "ä½¿ç”¨å›è¡€ä¸¹",
            "ä¸å•†äººå¯¹è¯",
            "è´­ä¹°æ³•å®",
            "æŒ‘æˆ˜é‡æ€ª",
            "è¿”å›åŸé•‡"
        ]
        
        for cmd in test_commands:
            # å¤„ç†å‘½ä»¤
            result = nlp_processor.process(cmd)
            assert result is not None
            assert 'normalized_command' in result
            
            # å‘é€åˆ°æœåŠ¡å™¨
            response = client.post('/api/game/command', json={
                'command': cmd
            })
            assert response.status_code == 200
            
            # éªŒè¯å“åº”
            data = response.get_json()
            assert 'success' in data or 'result' in data
    
    @pytest.mark.slow
    def test_long_conversation(self, nlp_processor):
        """æµ‹è¯•é•¿å¯¹è¯ï¼ˆ100+ è½®ï¼‰"""
        conversation_rounds = 120
        context = []
        memory_usage_start = self._get_memory_usage()
        
        for i in range(conversation_rounds):
            # ç”Ÿæˆå¯¹è¯
            if i % 10 == 0:
                command = "æŸ¥çœ‹çŠ¶æ€"
            elif i % 5 == 0:
                command = f"æ¢ç´¢ç¬¬{i//5}å±‚ç§˜å¢ƒ"
            else:
                command = f"ä¸NPC_{i}å¯¹è¯"
            
            # å¤„ç†å‘½ä»¤
            start_time = time.time()
            result = nlp_processor.process(command, context)
            process_time = time.time() - start_time
            
            # éªŒè¯ç»“æœ
            assert result is not None
            assert process_time < 5.0  # å¤„ç†æ—¶é—´åº”å°äº5ç§’
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            context.append({
                'command': command,
                'result': result
            })
            
            # æ¯20è½®æ£€æŸ¥ä¸€æ¬¡å†…å­˜
            if i % 20 == 0:
                gc.collect()
                memory_usage = self._get_memory_usage()
                memory_growth = memory_usage - memory_usage_start
                
                # å†…å­˜å¢é•¿åº”è¯¥æ˜¯åˆç†çš„
                assert memory_growth < 100 * 1024 * 1024  # å°äº100MB
                
                # éªŒè¯ä¸Šä¸‹æ–‡å‹ç¼©æ˜¯å¦å·¥ä½œ
                if hasattr(nlp_processor, 'context_compressor'):
                    compressed = nlp_processor.context_compressor.get_context()
                    compressed_size = len(str(compressed))
                    original_size = len(str(context))
                    compression_ratio = compressed_size / original_size if original_size > 0 else 1
                    assert compression_ratio < 0.8  # è‡³å°‘20%çš„å‹ç¼©ç‡
    
    @pytest.mark.slow
    def test_concurrent_users(self, app):
        """æµ‹è¯•å¹¶å‘ç”¨æˆ·ï¼ˆ10+ å¹¶å‘ï¼‰"""
        from xwe.core.nlp.nlp_processor import NLPProcessor
        
        concurrent_users = 15
        commands_per_user = 20
        
        def simulate_user(user_id: int) -> Dict[str, Any]:
            """æ¨¡æ‹Ÿå•ä¸ªç”¨æˆ·çš„è¡Œä¸º"""
            processor = NLPProcessor(use_context_compression=True)
            results = []
            errors = []
            
            for i in range(commands_per_user):
                try:
                    # ç”Ÿæˆéšæœºå‘½ä»¤
                    commands = [
                        f"ç”¨æˆ·{user_id}æ¢ç´¢åŒºåŸŸ{i}",
                        f"ç”¨æˆ·{user_id}æˆ˜æ–—",
                        f"ç”¨æˆ·{user_id}ä¿®ç‚¼",
                        f"ç”¨æˆ·{user_id}æŸ¥çœ‹èƒŒåŒ…"
                    ]
                    command = commands[i % len(commands)]
                    
                    # å¤„ç†å‘½ä»¤
                    start_time = time.time()
                    result = processor.process(command)
                    duration = time.time() - start_time
                    
                    results.append({
                        'command': command,
                        'duration': duration,
                        'success': True
                    })
                    
                except Exception as e:
                    errors.append(str(e))
                    results.append({
                        'command': command,
                        'duration': 0,
                        'success': False,
                        'error': str(e)
                    })
            
            return {
                'user_id': user_id,
                'results': results,
                'errors': errors
            }
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘æµ‹è¯•
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = []
            start_time = time.time()
            
            for i in range(concurrent_users):
                future = executor.submit(simulate_user, i)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            all_results = []
            for future in as_completed(futures):
                result = future.result()
                all_results.append(result)
            
            total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        total_commands = 0
        successful_commands = 0
        total_errors = 0
        avg_duration = 0
        
        for user_result in all_results:
            for cmd_result in user_result['results']:
                total_commands += 1
                if cmd_result['success']:
                    successful_commands += 1
                    avg_duration += cmd_result['duration']
                else:
                    total_errors += 1
        
        if successful_commands > 0:
            avg_duration /= successful_commands
        
        # éªŒè¯ç»“æœ
        success_rate = successful_commands / total_commands if total_commands > 0 else 0
        assert success_rate > 0.95  # 95%ä»¥ä¸Šçš„æˆåŠŸç‡
        assert avg_duration < 1.0  # å¹³å‡å¤„ç†æ—¶é—´å°äº1ç§’
        assert total_time < 60  # æ€»æ—¶é—´å°äº60ç§’
        
        print(f"\nå¹¶å‘æµ‹è¯•ç»“æœ:")
        print(f"  æ€»å‘½ä»¤æ•°: {total_commands}")
        print(f"  æˆåŠŸç‡: {success_rate * 100:.2f}%")
        print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_duration * 1000:.2f}ms")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    def test_error_recovery(self, nlp_processor):
        """æµ‹è¯•é”™è¯¯æ¢å¤"""
        # 1. æµ‹è¯•æ— æ•ˆè¾“å…¥æ¢å¤
        invalid_inputs = [
            "",  # ç©ºè¾“å…¥
            " " * 1000,  # è¶…é•¿ç©ºæ ¼
            "âŒğŸ”¥ğŸ’€" * 100,  # å¤§é‡è¡¨æƒ…ç¬¦å·
            "\n" * 50,  # å¤§é‡æ¢è¡Œ
            "a" * 10000,  # è¶…é•¿è¾“å…¥
        ]
        
        for invalid_input in invalid_inputs:
            try:
                result = nlp_processor.process(invalid_input)
                # åº”è¯¥è¿”å›åˆç†çš„é»˜è®¤ç»“æœ
                assert result is not None
                assert 'normalized_command' in result
            except Exception as e:
                # ä¸åº”è¯¥å´©æºƒ
                pytest.fail(f"å¤„ç†æ— æ•ˆè¾“å…¥æ—¶å´©æºƒ: {e}")
        
        # 2. æµ‹è¯• API è¶…æ—¶æ¢å¤
        original_timeout = os.environ.get('LLM_TIMEOUT', '30')
        os.environ['LLM_TIMEOUT'] = '0.001'  # è®¾ç½®æçŸ­è¶…æ—¶
        
        try:
            # å³ä½¿è¶…æ—¶ä¹Ÿåº”è¯¥è¿”å›ç»“æœ
            result = nlp_processor.process("æµ‹è¯•è¶…æ—¶æ¢å¤")
            assert result is not None
        finally:
            os.environ['LLM_TIMEOUT'] = original_timeout
        
        # 3. æµ‹è¯•å†…å­˜å‹åŠ›æ¢å¤
        large_context = [{"content": "x" * 1000} for _ in range(1000)]
        try:
            result = nlp_processor.process("æµ‹è¯•å¤§ä¸Šä¸‹æ–‡", large_context)
            assert result is not None
        except MemoryError:
            # åº”è¯¥ä¼˜é›…å¤„ç†å†…å­˜é”™è¯¯
            pass
    
    @pytest.mark.slow
    def test_memory_leak_detection(self, nlp_processor):
        """å†…å­˜æ³„æ¼æ£€æµ‹"""
        # å¯åŠ¨å†…å­˜è¿½è¸ª
        tracemalloc.start()
        
        # è·å–åˆå§‹å¿«ç…§
        snapshot1 = tracemalloc.take_snapshot()
        
        # æ‰§è¡Œå¤§é‡æ“ä½œ
        for i in range(100):
            context = [{"round": j, "data": "test" * 100} for j in range(50)]
            result = nlp_processor.process(f"æµ‹è¯•å‘½ä»¤ {i}", context)
            
            # æ¯10è½®å¼ºåˆ¶åƒåœ¾å›æ”¶
            if i % 10 == 0:
                gc.collect()
        
        # è·å–ç»“æŸå¿«ç…§
        snapshot2 = tracemalloc.take_snapshot()
        
        # æ¯”è¾ƒå·®å¼‚
        top_stats = snapshot2.compare_to(snapshot1, 'lineno')
        
        # åˆ†æå†…å­˜å¢é•¿
        total_growth = 0
        suspicious_files = []
        
        for stat in top_stats[:10]:
            growth = stat.size_diff
            total_growth += growth
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç–‘çš„å†…å­˜å¢é•¿
            if growth > 10 * 1024 * 1024:  # 10MB
                suspicious_files.append({
                    'file': stat.traceback.format()[0],
                    'growth': growth / 1024 / 1024  # è½¬æ¢ä¸ºMB
                })
        
        # åœæ­¢è¿½è¸ª
        tracemalloc.stop()
        
        # éªŒè¯
        assert total_growth < 50 * 1024 * 1024  # æ€»å¢é•¿å°äº50MB
        
        if suspicious_files:
            print("\nå¯ç–‘çš„å†…å­˜å¢é•¿:")
            for file_info in suspicious_files:
                print(f"  {file_info['file']}: {file_info['growth']:.2f}MB")
    
    @pytest.mark.asyncio
    async def test_async_operations(self):
        """æµ‹è¯•å¼‚æ­¥æ“ä½œ"""
        from xwe.core.nlp.llm_client import LLMClient
        
        client = LLMClient()
        
        # æµ‹è¯•å¼‚æ­¥å¹¶å‘
        tasks = []
        for i in range(10):
            task = client.chat_async(f"å¼‚æ­¥æµ‹è¯•æ¶ˆæ¯ {i}")
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        # éªŒè¯ç»“æœ
        assert len(results) == 10
        assert all(r is not None for r in results)
        assert total_time < 5.0  # å¹¶å‘åº”è¯¥æ¯”ä¸²è¡Œå¿«
        
        # æ¸…ç†
        client.cleanup()
    
    def _get_memory_usage(self) -> int:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆå­—èŠ‚ï¼‰"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss


class TestSystemIntegration:
    """ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    
    def test_full_system_workflow(self, app):
        """æµ‹è¯•å®Œæ•´çš„ç³»ç»Ÿå·¥ä½œæµ"""
        with app.test_client() as client:
            # 1. å¥åº·æ£€æŸ¥
            response = client.get('/health')
            assert response.status_code in [200, 404]
            
            # 2. æŒ‡æ ‡ç«¯ç‚¹
            response = client.get('/metrics')
            assert response.status_code == 200
            metrics_data = response.data.decode('utf-8')
            assert 'xwe_nlp_request_seconds' in metrics_data
            
            # 3. æ¸¸æˆæµç¨‹
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„é›†æˆæµ‹è¯•
    
    def test_configuration_changes(self):
        """æµ‹è¯•é…ç½®å˜æ›´"""
        configs = [
            {'USE_MOCK_LLM': 'true', 'ENABLE_CONTEXT_COMPRESSION': 'true'},
            {'USE_MOCK_LLM': 'true', 'ENABLE_CONTEXT_COMPRESSION': 'false'},
            {'USE_MOCK_LLM': 'false', 'ENABLE_CONTEXT_COMPRESSION': 'true'},
        ]
        
        for config in configs:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            for key, value in config.items():
                os.environ[key] = value
            
            # åˆ›å»ºæ–°çš„å¤„ç†å™¨
            from xwe.core.nlp.nlp_processor import NLPProcessor
            processor = NLPProcessor()
            
            # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
            result = processor.process("æµ‹è¯•é…ç½®")
            assert result is not None


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
