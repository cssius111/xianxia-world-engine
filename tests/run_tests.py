#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è¿è¡Œå™¨
æ–¹ä¾¿è¿è¡Œä¸åŒç±»å‹çš„æµ‹è¯•
"""

import os
import sys
import argparse
import subprocess
import time
from pathlib import Path


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_dir = self.project_root / 'tests'
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['PYTHONPATH'] = str(self.project_root / 'src')
        os.environ['USE_MOCK_LLM'] = 'true'
        os.environ['ENABLE_PROMETHEUS'] = 'true'
        os.environ['ENABLE_CONTEXT_COMPRESSION'] = 'true'
    
    def run_unit_tests(self, module=None):
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        print("=" * 60)
        print("è¿è¡Œå•å…ƒæµ‹è¯•...")
        print("=" * 60)
        
        if module:
            cmd = ['pytest', f'tests/unit/test_{module}.py', '-v']
        else:
            cmd = ['pytest', 'tests/unit', '-v']
        
        return self._run_command(cmd)
    
    def run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        print("=" * 60)
        print("è¿è¡Œé›†æˆæµ‹è¯•...")
        print("=" * 60)
        
        cmd = ['pytest', 'tests/integration', '-v', '--tb=short']
        return self._run_command(cmd)
    
    def run_e2e_tests(self):
        """è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
        print("=" * 60)
        print("è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•...")
        print("=" * 60)
        
        cmd = ['pytest', 'tests/e2e', '-v', '--tb=short', '-s']
        return self._run_command(cmd)
    
    def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("=" * 60)
        print("è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        print("=" * 60)
        
        cmd = ['pytest', 'tests/benchmark', '-v', '--benchmark-only']
        return self._run_command(cmd)
    
    def run_stress_tests(self):
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        print("=" * 60)
        print("è¿è¡Œå‹åŠ›æµ‹è¯•...")
        print("=" * 60)
        
        cmd = ['pytest', 'tests/stress', '-v', '-s', '--tb=short']
        return self._run_command(cmd)
    
    def run_regression_tests(self):
        """è¿è¡Œå›å½’æµ‹è¯•"""
        print("=" * 60)
        print("è¿è¡Œå›å½’æµ‹è¯•...")
        print("=" * 60)
        
        cmd = ['pytest', 'tests/regression', '-v', '--tb=short']
        return self._run_command(cmd)
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 60)
        print("è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶...")
        print("=" * 60)
        
        test_suites = [
            ('å•å…ƒæµ‹è¯•', lambda: self.run_unit_tests()),
            ('é›†æˆæµ‹è¯•', lambda: self.run_integration_tests()),
            ('å›å½’æµ‹è¯•', lambda: self.run_regression_tests()),
        ]
        
        results = {}
        
        for name, test_func in test_suites:
            print(f"\n>>> å¼€å§‹ {name}")
            start_time = time.time()
            
            result = test_func()
            elapsed = time.time() - start_time
            
            results[name] = {
                'passed': result == 0,
                'duration': elapsed
            }
            
            status = "âœ… é€šè¿‡" if result == 0 else "âŒ å¤±è´¥"
            print(f"<<< {name} {status} (è€—æ—¶: {elapsed:.2f}ç§’)\n")
        
        # æ‰“å°æ€»ç»“
        self._print_summary(results)
        
        # å¦‚æœæœ‰å¤±è´¥ï¼Œè¿”å›éé›¶é€€å‡ºç 
        return 0 if all(r['passed'] for r in results.values()) else 1
    
    def run_quick_tests(self):
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆå•å…ƒæµ‹è¯•å­é›†ï¼‰"""
        print("=" * 60)
        print("è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        print("=" * 60)
        
        # åªè¿è¡Œæ ‡è®°ä¸º quick çš„æµ‹è¯•
        cmd = ['pytest', 'tests/unit', '-v', '-m', 'quick', '--tb=short']
        return self._run_command(cmd)
    
    def run_coverage(self):
        """è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
        print("=" * 60)
        print("è¿è¡Œæµ‹è¯•è¦†ç›–ç‡åˆ†æ...")
        print("=" * 60)
        
        cmd = [
            'pytest',
            'tests/unit',
            'tests/integration',
            '--cov=src/xwe',
            '--cov-report=html',
            '--cov-report=term',
            '-v'
        ]
        
        result = self._run_command(cmd)
        
        if result == 0:
            print("\nè¦†ç›–ç‡æŠ¥å‘Šå·²ç”Ÿæˆ: htmlcov/index.html")
        
        return result
    
    def run_specific_test(self, test_path):
        """è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶æˆ–å‡½æ•°"""
        print("=" * 60)
        print(f"è¿è¡Œç‰¹å®šæµ‹è¯•: {test_path}")
        print("=" * 60)
        
        cmd = ['pytest', test_path, '-v', '-s']
        return self._run_command(cmd)
    
    def clean_test_artifacts(self):
        """æ¸…ç†æµ‹è¯•äº§ç‰©"""
        print("æ¸…ç†æµ‹è¯•äº§ç‰©...")
        
        artifacts = [
            '.pytest_cache',
            'htmlcov',
            '.coverage',
            '*.pyc',
            '__pycache__',
            '.benchmarks'
        ]
        
        for artifact in artifacts:
            cmd = ['find', '.', '-name', artifact, '-exec', 'rm', '-rf', '{}', '+']
            subprocess.run(cmd, cwd=self.project_root)
        
        print("æ¸…ç†å®Œæˆ")
    
    def _run_command(self, cmd):
        """æ‰§è¡Œå‘½ä»¤"""
        print(f"æ‰§è¡Œ: {' '.join(cmd)}")
        print("-" * 60)
        
        result = subprocess.run(
            cmd,
            cwd=self.project_root,
            env=os.environ.copy()
        )
        
        return result.returncode
    
    def _print_summary(self, results):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        
        total_duration = sum(r['duration'] for r in results.values())
        passed_count = sum(1 for r in results.values() if r['passed'])
        total_count = len(results)
        
        print(f"\næ€»æµ‹è¯•å¥—ä»¶æ•°: {total_count}")
        print(f"é€šè¿‡: {passed_count}")
        print(f"å¤±è´¥: {total_count - passed_count}")
        print(f"æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        
        print("\nè¯¦ç»†ç»“æœ:")
        for name, result in results.items():
            status = "âœ…" if result['passed'] else "âŒ"
            print(f"  {status} {name}: {result['duration']:.2f}ç§’")
        
        if passed_count == total_count:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='XianXia World Engine æµ‹è¯•è¿è¡Œå™¨')
    
    parser.add_argument(
        'test_type',
        choices=[
            'unit', 'integration', 'e2e', 'performance', 
            'stress', 'regression', 'all', 'quick', 
            'coverage', 'clean'
        ],
        help='æµ‹è¯•ç±»å‹'
    )
    
    parser.add_argument(
        '--module',
        help='ç‰¹å®šçš„æ¨¡å—åï¼ˆç”¨äºå•å…ƒæµ‹è¯•ï¼‰'
    )
    
    parser.add_argument(
        '--test',
        help='ç‰¹å®šçš„æµ‹è¯•è·¯å¾„ï¼ˆä¾‹å¦‚: tests/unit/test_nlp_processor.py::TestNLPProcessor::test_initializationï¼‰'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )
    
    parser.add_argument(
        '--fail-fast',
        action='store_true',
        help='é‡åˆ°ç¬¬ä¸€ä¸ªå¤±è´¥å°±åœæ­¢'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®é¢å¤–çš„ç¯å¢ƒå˜é‡
    if args.verbose:
        os.environ['VERBOSE_LOG'] = 'true'
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = TestRunner()
    
    # æ ¹æ®æµ‹è¯•ç±»å‹æ‰§è¡Œ
    if args.test:
        return runner.run_specific_test(args.test)
    
    if args.test_type == 'unit':
        return runner.run_unit_tests(args.module)
    elif args.test_type == 'integration':
        return runner.run_integration_tests()
    elif args.test_type == 'e2e':
        return runner.run_e2e_tests()
    elif args.test_type == 'performance':
        return runner.run_performance_tests()
    elif args.test_type == 'stress':
        return runner.run_stress_tests()
    elif args.test_type == 'regression':
        return runner.run_regression_tests()
    elif args.test_type == 'all':
        return runner.run_all_tests()
    elif args.test_type == 'quick':
        return runner.run_quick_tests()
    elif args.test_type == 'coverage':
        return runner.run_coverage()
    elif args.test_type == 'clean':
        runner.clean_test_artifacts()
        return 0


if __name__ == '__main__':
    sys.exit(main())
